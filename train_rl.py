"""
train_rl.py - ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

MaskablePPOë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ì „ëµ í•™ìŠµ
"""

import os
import random
import numpy as np
import torch
from datetime import datetime
import matplotlib.pyplot as plt


def set_seed(seed=42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ seed ê³ ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸ² Seed ê³ ì •: {seed}")


# Stable Baselines 3
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# MaskablePPO (Action Masking ì§€ì›)
try:
    from sb3_contrib import MaskablePPO
    from sb3_contrib.common.wrappers import ActionMasker
    MASKABLE_AVAILABLE = True
except ImportError:
    MASKABLE_AVAILABLE = False
    print("âš ï¸ sb3-contribê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sb3-contrib")

# í™˜ê²½
from apple_env import AppleGameEnv, AppleGameEnvWithMask

# ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ ë¹„êµìš©
from main import (
    read_matrix, find_candidates_fast, apply_move_fast,
    solve_pair_first, solve_full_rollout
)


class LoggingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥ ë° ê³ ì • ë³´ë“œ í‰ê°€ ì½œë°±"""
    
    def __init__(self, log_freq=10000, eval_freq=10000, board_dir="board_mat", verbose=1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.eval_freq = eval_freq  # ê³ ì • ë³´ë“œ í‰ê°€ ì£¼ê¸°
        self.board_dir = board_dir
        self.episode_scores = []
        
        # timestep ê¸°ì¤€ ì¹´ìš´í„°
        self._next_log_timestep = log_freq
        self._next_eval_timestep = eval_freq
        
        # ê·¸ë˜í”„ìš© ë°ì´í„° (ê³ ì • ë³´ë“œ í‰ê°€ ê²°ê³¼)
        self.timesteps_history = []
        self.fixed_board_avg_history = []  # ê³ ì • ë³´ë“œ í‰ê·  ì ìˆ˜
        
        # ê³ ì • ë³´ë“œ íŒŒì¼ ë¡œë“œ
        self.board_files = sorted([
            os.path.join(board_dir, f) 
            for f in os.listdir(board_dir) 
            if f.endswith(".txt")
        ])
        print(f"ğŸ“‹ ê³ ì • í‰ê°€ìš© ë³´ë“œ: {len(self.board_files)}ê°œ")
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸°ë¡ (ë¡œê·¸ìš©)
        if self.locals.get("dones") is not None:
            for i, done in enumerate(self.locals["dones"]):
                if done:
                    info = self.locals["infos"][i]
                    if "total_score" in info:
                        self.episode_scores.append(info["total_score"])
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ë¡œê·¸ ì¶œë ¥ (num_timesteps ê¸°ì¤€)
        if self.num_timesteps >= self._next_log_timestep and self.episode_scores:
            avg_score = np.mean(self.episode_scores[-100:])
            print(f"[Step {self.num_timesteps:,}] ìµœê·¼ 100 ì—í”¼ì†Œë“œ í‰ê· : {avg_score:.1f}")
            self._next_log_timestep += self.log_freq
        
        # ê³ ì • ë³´ë“œ í‰ê°€ (num_timesteps ê¸°ì¤€)
        if self.num_timesteps >= self._next_eval_timestep:
            fixed_avg = self._evaluate_on_fixed_boards()
            self.timesteps_history.append(self.num_timesteps)
            self.fixed_board_avg_history.append(fixed_avg)
            print(f"[Step {self.num_timesteps:,}] ğŸ¯ ê³ ì • ë³´ë“œ í‰ê· : {fixed_avg:.1f}")
            self._next_eval_timestep += self.eval_freq
        
        return True
    
    def _evaluate_on_fixed_boards(self):
        """ê³ ì • ë³´ë“œ ì „ì²´ì—ì„œ í˜„ì¬ ëª¨ë¸ í‰ê°€"""
        scores = []
        
        for board_path in self.board_files:
            mat = read_matrix(board_path)
            
            # í‰ê°€ìš© í™˜ê²½ ìƒì„± (seed=Noneìœ¼ë¡œ í•™ìŠµ RNGì— ì˜í–¥ ì—†ìŒ)
            env = make_env(use_mask=True)()
            env.reset()  # Monitorê°€ step í—ˆìš©í•˜ë„ë¡ reset í•„ìš”
            
            # ë³´ë“œ êµì²´ (reset ì§í›„ ë®ì–´ì“°ê¸°)
            unwrapped = env.unwrapped
            unwrapped.board = mat.copy().astype(np.int32)
            unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
            unwrapped.total_score = 0
            unwrapped.steps = 0
            unwrapped._compute_next_candidates_cache()
            obs = unwrapped._get_obs()
            
            # í”Œë ˆì´
            while True:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                if terminated or truncated:
                    break
            
            scores.append(info["total_score"])
        
        return np.mean(scores)
    
    def plot_learning_curve(self, save_path="learning_curve.png", show=True):
        """í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„± (ê³ ì • ë³´ë“œ í‰ê°€ ê¸°ì¤€)"""
        import matplotlib as mpl
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
        mpl.rcParams["font.family"] = "Malgun Gothic"
        mpl.rcParams["axes.unicode_minus"] = False
        
        if not self.timesteps_history:
            print("âš ï¸ ê¸°ë¡ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        plt.figure(figsize=(10, 6))
        
        # ê³ ì • ë³´ë“œ í‰ê·  ì ìˆ˜
        plt.plot(self.timesteps_history, self.fixed_board_avg_history, 
                 'b-o', linewidth=2, markersize=4, label=f'ê³ ì • ë³´ë“œ í‰ê·  ({len(self.board_files)}ê°œ)')
        
        plt.xlabel('Timesteps', fontsize=12)
        plt.ylabel('Average Score', fontsize=12)
        plt.title('í•™ìŠµ ì¤‘ ê³ ì • ë³´ë“œ í‰ê·  ì ìˆ˜ ë³€í™”', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # Yì¶• ë²”ìœ„ ì¡°ì • (ë³€í™”ë¥¼ ë” ì˜ ë³´ì´ê²Œ)
        if self.fixed_board_avg_history:
            y_min = min(self.fixed_board_avg_history) - 5
            y_max = max(self.fixed_board_avg_history) + 5
            plt.ylim(y_min, y_max)
        
        plt.tight_layout()
        
        # ì €ì¥
        plt.savefig(save_path, dpi=150)
        print(f"ğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()


def mask_fn(env):
    """í™˜ê²½ì—ì„œ action maskë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    return env.get_action_mask()


def make_env(board_dir="board_mat", rank=0, use_mask=False, seed=None):
    """í™˜ê²½ ìƒì„± í•¨ìˆ˜
    
    seedê°€ ì£¼ì–´ì§€ë©´ base_seedë¡œ ì„¤ì •ë˜ì–´
    ë§¤ ì—í”¼ì†Œë“œ resetë§ˆë‹¤ base_seed + episode_idxë¡œ ì¼ê´€ëœ seed ì‚¬ìš©
    """
    def _init():
        # base_seed ê³„ì‚°: seedê°€ ìˆìœ¼ë©´ seed + rank
        base_seed = (seed + rank) if seed is not None else None
        
        if use_mask:
            env = AppleGameEnvWithMask(board_dir=board_dir, base_seed=base_seed)
            env = ActionMasker(env, mask_fn)
        else:
            env = AppleGameEnv(board_dir=board_dir, base_seed=base_seed)
        
        return Monitor(env)
    return _init


def train_ppo(
    total_timesteps=100000,
    learning_rate=5e-5,
    n_steps=2048,
    batch_size=128,
    n_epochs=10,
    n_envs=2,
    save_path="models/ppo_apple"
):
    """MaskablePPO í•™ìŠµ (Action Masking ì ìš©)"""
    print("=" * 60)
    print("ğŸ§  MaskablePPO í•™ìŠµ ì‹œì‘ (Action Masking ì ìš©)")
    print("=" * 60)
    
    if not MASKABLE_AVAILABLE:
        print("âŒ sb3-contribê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sb3-contrib")
        return None
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (Mask ì§€ì› í™˜ê²½) - ê° í™˜ê²½ì— rankë³„ ì‹œë“œ ì ìš©
    env = DummyVecEnv([make_env(rank=i, use_mask=True, seed=42) for i in range(n_envs)])
    
    # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ­ Action Masking: í™œì„±í™” (ìœ íš¨í•œ í›„ë³´ë§Œ ì„ íƒ ê°€ëŠ¥)")
    
    # MaskablePPO ëª¨ë¸ ìƒì„± (Dict observation â†’ MultiInputPolicy)
    model = MaskablePPO(
        "MultiInputPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.999,
        gae_lambda=0.99,
        clip_range=0.2,
        verbose=1,
        device=device,
        seed=42  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
    )
    
    # ì½œë°± ì„¤ì • (ê³ ì • ë³´ë“œ í‰ê°€ ì£¼ê¸°: total_timesteps / 50 ë˜ëŠ” ìµœì†Œ 5000)
    eval_freq = 5000
    callback = LoggingCallback(log_freq=10000, eval_freq=eval_freq)
    
    # í•™ìŠµ
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì • (í™˜ê²½ {n_envs}ê°œ ë³‘ë ¬)")
    print(f"ğŸ“Š ê³ ì • ë³´ë“œ í‰ê°€ ì£¼ê¸°: {eval_freq:,} ìŠ¤í…ë§ˆë‹¤")
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback
    )
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")
    
    # í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„±
    callback.plot_learning_curve(
        save_path=save_path.replace(".zip", "") + "_learning_curve.png",
        show=True
    )
    
    return model


def evaluate_model_on_all_boards(model, env_factory, board_dir="board_mat", verbose=False):
    """
    board_matì˜ ëª¨ë“  ë³´ë“œì—ì„œ í‰ê°€ (ê³ ì • ë³´ë“œ ì „ì²´ í‰ê°€)
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        env_factory: í™˜ê²½ ìƒì„± í•¨ìˆ˜
        board_dir: ë³´ë“œ íŒŒì¼ ë””ë ‰í† ë¦¬
        verbose: ê° ë³´ë“œë³„ ì ìˆ˜ ì¶œë ¥ ì—¬ë¶€
    """
    # ëª¨ë“  ë³´ë“œ íŒŒì¼ ë¡œë“œ
    board_files = sorted([
        os.path.join(board_dir, f) 
        for f in os.listdir(board_dir) 
        if f.endswith(".txt")
    ])
    
    if not board_files:
        print(f"âš ï¸ {board_dir}ì— ë³´ë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    scores = []
    steps_list = []
    
    for board_path in board_files:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # í™˜ê²½ ìƒì„± ë° ì´ˆê¸°í™”
        env = env_factory()
        env.reset()
        
        # ë³´ë“œ êµì²´
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        # í”Œë ˆì´
        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated or truncated:
                break
        
        scores.append(info["total_score"])
        steps_list.append(info["steps"])
        
        if verbose:
            print(f"  {board_name}: {info['total_score']} ì ")
    
    return {
        "mean_score": np.mean(scores),
        "std_score": np.std(scores),
        "max_score": np.max(scores),
        "min_score": np.min(scores),
        "mean_steps": np.mean(steps_list),
        "n_boards": len(board_files),
        "scores": scores
    }


def compare_with_heuristics(model, board_dir="board_mat", verbose=True):
    """í•™ìŠµëœ ëª¨ë¸ê³¼ ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ë¹„êµ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š RL ëª¨ë¸ vs íœ´ë¦¬ìŠ¤í‹± ë¹„êµ")
    print("=" * 60)
    
    board_files = sorted([
        os.path.join(board_dir, f) 
        for f in os.listdir(board_dir) 
        if f.endswith(".txt")
    ])
    
    results = {
        "RL Model": [],
        "Pair-First": [],
        "Full-Rollout": []
    }
    
    for board_path in board_files:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # RL ëª¨ë¸
        env = make_env(use_mask=True)()
        env.reset()
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        while unwrapped.candidates:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated:
                break
        results["RL Model"].append(info["total_score"])
        
        # Pair-First
        _, score_pf, _ = solve_pair_first(mat.copy(), verbose=False)
        results["Pair-First"].append(score_pf)
        
        # Full-Rollout
        _, score_fr, _ = solve_full_rollout(mat.copy(), top_k=30, verbose=False)
        results["Full-Rollout"].append(score_fr)
        
        if verbose:
            print(f"\n[{board_name}]")
            print(f"  RL Model:     {results['RL Model'][-1]:>4}")
            print(f"  Pair-First:   {results['Pair-First'][-1]:>4}")
            print(f"  Full-Rollout: {results['Full-Rollout'][-1]:>4}")
    
    # í‰ê·  ì¶œë ¥
    print("\n" + "-" * 60)
    print("ğŸ“ˆ í‰ê·  ì ìˆ˜:")
    for name, scores in results.items():
        print(f"  {name:<15}: {np.mean(scores):.1f} (Â±{np.std(scores):.1f})")
    
    return results


def play_with_model(model, board_path=None, render=True):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´"""
    env = make_env(use_mask=True)()
    unwrapped = env.unwrapped
    unwrapped.render_mode = "human" if render else None
    
    obs, _ = env.reset()
    
    if board_path:
        unwrapped.board = read_matrix(board_path).astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
    
    if render:
        unwrapped.render()
    
    while True:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        
        if render:
            unwrapped.render()
        
        if terminated or truncated:
            break
    
    print(f"\nğŸ ê²Œì„ ì¢…ë£Œ! ìµœì¢… ì ìˆ˜: {info['total_score']}, ìŠ¤í…: {info['steps']}")
    return info


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ")
    parser.add_argument("--mode", type=str, default="train", 
                        choices=["train", "eval", "play", "compare"])
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--model", type=str, default=None, help="í‰ê°€/í”Œë ˆì´í•  ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--board", type=str, default=None, help="íŠ¹ì • ë³´ë“œë¡œ í”Œë ˆì´")
    parser.add_argument("--seed", type=int, default=42, help="ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ (ê¸°ë³¸: 42, -1ì´ë©´ ëœë¤)")
    args = parser.parse_args()
    
    # Seed ê³ ì • (-1ì´ë©´ ëœë¤)
    if args.seed >= 0:
        set_seed(args.seed)
    else:
        print("ğŸ² Seed: ëœë¤ (ê³ ì • ì•ˆí•¨)")
    
    if args.mode == "train":
        # í•™ìŠµ (MaskablePPO)
        print("=" * 60)
        print("ğŸ§  MaskablePPO í•™ìŠµ")
        print("=" * 60)
        model = train_ppo(total_timesteps=args.timesteps)
        
        # í•™ìŠµ í›„ í‰ê°€ (ê³ ì • ë³´ë“œ ì „ì²´ í‰ê°€)
        print("\nğŸ“Š í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ì¤‘ (board_mat ì „ì²´ ë³´ë“œ)...")
        results = evaluate_model_on_all_boards(
            model, 
            lambda: make_env(use_mask=True)(),
            verbose=True
        )
        print(f"\nğŸ“ˆ ì „ì²´ {results['n_boards']}ê°œ ë³´ë“œ í‰ê· : {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"   ìµœê³ : {results['max_score']}, ìµœì €: {results['min_score']}")
        
        # íœ´ë¦¬ìŠ¤í‹±ê³¼ ë¹„êµ
        compare_with_heuristics(model)
        
    elif args.mode == "eval":
        # í‰ê°€ë§Œ (ê³ ì • ë³´ë“œ ì „ì²´ í‰ê°€)
        model_path = args.model or "models/ppo_apple"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘ (board_mat ì „ì²´ ë³´ë“œ)...")
        results = evaluate_model_on_all_boards(
            model, 
            lambda: make_env(use_mask=True)(),
            verbose=True
        )
        print(f"\nğŸ“ˆ ì „ì²´ {results['n_boards']}ê°œ ë³´ë“œ í‰ê· : {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"   ìµœê³ : {results['max_score']}, ìµœì €: {results['min_score']}")
        
    elif args.mode == "play":
        # í”Œë ˆì´
        model_path = args.model or "models/ppo_apple"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        board_path = f"board_mat/{args.board}.txt" if args.board else None
        play_with_model(model, board_path, render=True)
        
    elif args.mode == "compare":
        # ë¹„êµ
        model_path = args.model or "models/ppo_apple"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        compare_with_heuristics(model)
