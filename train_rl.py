"""
train_rl.py - ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

MaskablePPOë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ì „ëµ í•™ìŠµ
"""

import os
import random
import numpy as np
import torch
from datetime import datetime


def set_seed(seed=42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ seed ê³ ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸ² Seed ê³ ì •: {seed}")


# Stable Baselines 3
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# MaskablePPO (Action Masking ì§€ì›)
try:
    from sb3_contrib import MaskablePPO
    from sb3_contrib.common.wrappers import ActionMasker
    MASKABLE_AVAILABLE = True
except ImportError:
    MASKABLE_AVAILABLE = False
    print("âš ï¸ sb3-contribê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sb3-contrib")

# í™˜ê²½
from apple_env import AppleGameEnv, AppleGameEnvWithMask

# ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ ë¹„êµìš©
from main import (
    read_matrix, find_candidates_fast, apply_move_fast,
    solve_pair_first, solve_full_rollout
)


class LoggingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥ ì½œë°±"""
    
    def __init__(self, log_freq=1000, verbose=1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.episode_rewards = []
        self.episode_scores = []
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸°ë¡
        if self.locals.get("dones") is not None:
            for i, done in enumerate(self.locals["dones"]):
                if done:
                    info = self.locals["infos"][i]
                    if "total_score" in info:
                        self.episode_scores.append(info["total_score"])
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì¶œë ¥
        if self.n_calls % self.log_freq == 0 and self.episode_scores:
            avg_score = np.mean(self.episode_scores[-100:])
            max_score = np.max(self.episode_scores[-100:]) if self.episode_scores else 0
            print(f"[Step {self.n_calls}] ìµœê·¼ 100 ì—í”¼ì†Œë“œ - í‰ê· : {avg_score:.1f}, ìµœê³ : {max_score}")
        
        return True


def mask_fn(env):
    """í™˜ê²½ì—ì„œ action maskë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    return env.get_action_mask()


def make_env(board_dir="board_mat", rank=0, use_mask=False):
    """í™˜ê²½ ìƒì„± í•¨ìˆ˜"""
    def _init():
        if use_mask:
            env = AppleGameEnvWithMask(board_dir=board_dir)
            env = ActionMasker(env, mask_fn)
        else:
            env = AppleGameEnv(board_dir=board_dir)
        env = Monitor(env)
        return env
    return _init


def train_ppo(
    total_timesteps=100000,
    learning_rate=0.0001,
    n_steps=2048,
    batch_size=128,
    n_epochs=10,
    n_envs=2,
    save_path="models/ppo_apple"
):
    """MaskablePPO í•™ìŠµ (Action Masking ì ìš©)"""
    print("=" * 60)
    print("ğŸ§  MaskablePPO í•™ìŠµ ì‹œì‘ (Action Masking ì ìš©)")
    print("=" * 60)
    
    if not MASKABLE_AVAILABLE:
        print("âŒ sb3-contribê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sb3-contrib")
        return None
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (Mask ì§€ì› í™˜ê²½)
    env = DummyVecEnv([make_env(rank=i, use_mask=True) for i in range(n_envs)])
    
    # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ­ Action Masking: í™œì„±í™” (ìœ íš¨í•œ í›„ë³´ë§Œ ì„ íƒ ê°€ëŠ¥)")
    
    # MaskablePPO ëª¨ë¸ ìƒì„±
    model = MaskablePPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.999,
        gae_lambda=0.99,
        clip_range=0.2,
        verbose=1,
        device=device
    )
    
    # ì½œë°± ì„¤ì •
    callback = LoggingCallback(log_freq=5000)
    
    # í•™ìŠµ
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì • (í™˜ê²½ {n_envs}ê°œ ë³‘ë ¬)")
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback
    )
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")
    
    return model


def evaluate_model_on_all_boards(model, env_factory, board_dir="board_mat", verbose=False):
    """
    board_matì˜ ëª¨ë“  ë³´ë“œì—ì„œ í‰ê°€ (ê³ ì • ë³´ë“œ ì „ì²´ í‰ê°€)
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        env_factory: í™˜ê²½ ìƒì„± í•¨ìˆ˜
        board_dir: ë³´ë“œ íŒŒì¼ ë””ë ‰í† ë¦¬
        verbose: ê° ë³´ë“œë³„ ì ìˆ˜ ì¶œë ¥ ì—¬ë¶€
    """
    # ëª¨ë“  ë³´ë“œ íŒŒì¼ ë¡œë“œ
    board_files = sorted([
        os.path.join(board_dir, f) 
        for f in os.listdir(board_dir) 
        if f.endswith(".txt")
    ])
    
    if not board_files:
        print(f"âš ï¸ {board_dir}ì— ë³´ë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    scores = []
    steps_list = []
    
    for board_path in board_files:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # í™˜ê²½ ìƒì„± ë° ì´ˆê¸°í™”
        env = env_factory()
        env.reset()
        
        # ë³´ë“œ êµì²´
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        # í”Œë ˆì´
        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated or truncated:
                break
        
        scores.append(info["total_score"])
        steps_list.append(info["steps"])
        
        if verbose:
            print(f"  {board_name}: {info['total_score']} ì ")
    
    return {
        "mean_score": np.mean(scores),
        "std_score": np.std(scores),
        "max_score": np.max(scores),
        "min_score": np.min(scores),
        "mean_steps": np.mean(steps_list),
        "n_boards": len(board_files),
        "scores": scores
    }


def compare_with_heuristics(model, board_dir="board_mat", verbose=True):
    """í•™ìŠµëœ ëª¨ë¸ê³¼ ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ë¹„êµ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š RL ëª¨ë¸ vs íœ´ë¦¬ìŠ¤í‹± ë¹„êµ")
    print("=" * 60)
    
    board_files = sorted([
        os.path.join(board_dir, f) 
        for f in os.listdir(board_dir) 
        if f.endswith(".txt")
    ])
    
    results = {
        "RL Model": [],
        "Pair-First": [],
        "Full-Rollout": []
    }
    
    for board_path in board_files:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # RL ëª¨ë¸
        env = make_env(use_mask=True)()
        env.reset()
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        while unwrapped.candidates:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated:
                break
        results["RL Model"].append(info["total_score"])
        
        # Pair-First
        _, score_pf, _ = solve_pair_first(mat.copy(), verbose=False)
        results["Pair-First"].append(score_pf)
        
        # Full-Rollout
        _, score_fr, _ = solve_full_rollout(mat.copy(), top_k=30, verbose=False)
        results["Full-Rollout"].append(score_fr)
        
        if verbose:
            print(f"\n[{board_name}]")
            print(f"  RL Model:     {results['RL Model'][-1]:>4}")
            print(f"  Pair-First:   {results['Pair-First'][-1]:>4}")
            print(f"  Full-Rollout: {results['Full-Rollout'][-1]:>4}")
    
    # í‰ê·  ì¶œë ¥
    print("\n" + "-" * 60)
    print("ğŸ“ˆ í‰ê·  ì ìˆ˜:")
    for name, scores in results.items():
        print(f"  {name:<15}: {np.mean(scores):.1f} (Â±{np.std(scores):.1f})")
    
    return results


def play_with_model(model, board_path=None, render=True):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´"""
    env = make_env(use_mask=True)()
    unwrapped = env.unwrapped
    unwrapped.render_mode = "human" if render else None
    
    obs, _ = env.reset()
    
    if board_path:
        unwrapped.board = read_matrix(board_path).astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
    
    if render:
        unwrapped.render()
    
    while True:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        
        if render:
            unwrapped.render()
        
        if terminated or truncated:
            break
    
    print(f"\nğŸ ê²Œì„ ì¢…ë£Œ! ìµœì¢… ì ìˆ˜: {info['total_score']}, ìŠ¤í…: {info['steps']}")
    return info


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ")
    parser.add_argument("--mode", type=str, default="train", 
                        choices=["train", "eval", "play", "compare"])
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--model", type=str, default=None, help="í‰ê°€/í”Œë ˆì´í•  ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--board", type=str, default=None, help="íŠ¹ì • ë³´ë“œë¡œ í”Œë ˆì´")
    parser.add_argument("--seed", type=int, default=42, help="ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ (ê¸°ë³¸: 42, -1ì´ë©´ ëœë¤)")
    args = parser.parse_args()
    
    # Seed ê³ ì • (-1ì´ë©´ ëœë¤)
    if args.seed >= 0:
        set_seed(args.seed)
    else:
        print("ğŸ² Seed: ëœë¤ (ê³ ì • ì•ˆí•¨)")
    
    if args.mode == "train":
        # í•™ìŠµ (MaskablePPO)
        print("=" * 60)
        print("ğŸ§  MaskablePPO í•™ìŠµ")
        print("=" * 60)
        model = train_ppo(total_timesteps=args.timesteps)
        
        # í•™ìŠµ í›„ í‰ê°€ (ê³ ì • ë³´ë“œ ì „ì²´ í‰ê°€)
        print("\nğŸ“Š í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ì¤‘ (board_mat ì „ì²´ ë³´ë“œ)...")
        results = evaluate_model_on_all_boards(
            model, 
            lambda: make_env(use_mask=True)(),
            verbose=True
        )
        print(f"\nğŸ“ˆ ì „ì²´ {results['n_boards']}ê°œ ë³´ë“œ í‰ê· : {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"   ìµœê³ : {results['max_score']}, ìµœì €: {results['min_score']}")
        
        # íœ´ë¦¬ìŠ¤í‹±ê³¼ ë¹„êµ
        compare_with_heuristics(model)
        
    elif args.mode == "eval":
        # í‰ê°€ë§Œ (ê³ ì • ë³´ë“œ ì „ì²´ í‰ê°€)
        model_path = args.model or "models/ppo_apple"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘ (board_mat ì „ì²´ ë³´ë“œ)...")
        results = evaluate_model_on_all_boards(
            model, 
            lambda: make_env(use_mask=True)(),
            verbose=True
        )
        print(f"\nğŸ“ˆ ì „ì²´ {results['n_boards']}ê°œ ë³´ë“œ í‰ê· : {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"   ìµœê³ : {results['max_score']}, ìµœì €: {results['min_score']}")
        
    elif args.mode == "play":
        # í”Œë ˆì´
        model_path = args.model or "models/ppo_apple"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        board_path = f"board_mat/{args.board}.txt" if args.board else None
        play_with_model(model, board_path, render=True)
        
    elif args.mode == "compare":
        # ë¹„êµ
        model_path = args.model or "models/ppo_apple"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        compare_with_heuristics(model)
