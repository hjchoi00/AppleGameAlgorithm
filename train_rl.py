"""
train_rl.py - ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

MaskablePPOë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ì „ëµ í•™ìŠµ
"""

import os
import numpy as np
import torch
from datetime import datetime

# Stable Baselines 3
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# MaskablePPO (Action Masking ì§€ì›)
try:
    from sb3_contrib import MaskablePPO
    from sb3_contrib.common.wrappers import ActionMasker
    from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback
    MASKABLE_AVAILABLE = True
except ImportError:
    MASKABLE_AVAILABLE = False
    print("âš ï¸ sb3-contribê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sb3-contrib")

# í™˜ê²½
from apple_env import AppleGameEnv, AppleGameEnvWithMask, AppleGameEnvTopK, AppleGameEnvTopKFlat

# ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ ë¹„êµìš©
from main import (
    read_matrix, find_candidates_fast, apply_move_fast,
    solve_pair_first, solve_full_rollout
)


class LoggingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥ ì½œë°±"""
    
    def __init__(self, log_freq=1000, verbose=1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.episode_rewards = []
        self.episode_scores = []
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸°ë¡
        if self.locals.get("dones") is not None:
            for i, done in enumerate(self.locals["dones"]):
                if done:
                    info = self.locals["infos"][i]
                    if "total_score" in info:
                        self.episode_scores.append(info["total_score"])
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì¶œë ¥
        if self.n_calls % self.log_freq == 0 and self.episode_scores:
            avg_score = np.mean(self.episode_scores[-100:])
            max_score = np.max(self.episode_scores[-100:]) if self.episode_scores else 0
            print(f"[Step {self.n_calls}] ìµœê·¼ 100 ì—í”¼ì†Œë“œ - í‰ê· : {avg_score:.1f}, ìµœê³ : {max_score}")
        
        return True


def mask_fn(env):
    """í™˜ê²½ì—ì„œ action maskë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    return env.get_action_mask()


def make_env(board_dir="board_mat", rank=0, use_mask=False):
    """í™˜ê²½ ìƒì„± í•¨ìˆ˜"""
    def _init():
        if use_mask:
            env = AppleGameEnvWithMask(board_dir=board_dir)
            # ActionMasker wrapper ì ìš© (ë§ˆìŠ¤í‚¹ ëª…ì‹œì  í™œì„±í™”)
            env = ActionMasker(env, mask_fn)
        else:
            env = AppleGameEnv(board_dir=board_dir)
        env = Monitor(env)
        return env
    return _init


def make_env_topk(board_dir="board_mat", rank=0, top_k=20):
    """Top-K í™˜ê²½ ìƒì„± í•¨ìˆ˜ (Flat ë²„ì „ - MLP Policy í˜¸í™˜)"""
    def _init():
        env = AppleGameEnvTopKFlat(board_dir=board_dir, top_k=top_k)
        # ActionMasker wrapper ì ìš©
        env = ActionMasker(env, mask_fn)
        env = Monitor(env)
        return env
    return _init


def train_ppo(
    total_timesteps=100000,
    learning_rate=3e-4,
    n_steps=4096,
    batch_size=256,
    n_epochs=10,
    n_envs=4,
    save_path="models/ppo_apple"
):
    """MaskablePPO í•™ìŠµ (Action Masking ì ìš©)"""
    print("=" * 60)
    print("ğŸ§  MaskablePPO í•™ìŠµ ì‹œì‘ (Action Masking ì ìš©)")
    print("=" * 60)
    
    if not MASKABLE_AVAILABLE:
        print("âŒ sb3-contribê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sb3-contrib")
        return None
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (Mask ì§€ì› í™˜ê²½)
    env = DummyVecEnv([make_env(rank=i, use_mask=True) for i in range(n_envs)])
    
    # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ­ Action Masking: í™œì„±í™” (ìœ íš¨í•œ í›„ë³´ë§Œ ì„ íƒ ê°€ëŠ¥)")
    
    # MaskablePPO ëª¨ë¸ ìƒì„±
    model = MaskablePPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.995,
        gae_lambda=0.97,
        clip_range=0.2,
        verbose=1,
        device=device
    )
    
    # ì½œë°± ì„¤ì •
    callback = LoggingCallback(log_freq=5000)
    
    # í•™ìŠµ
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì • (í™˜ê²½ {n_envs}ê°œ ë³‘ë ¬)")
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback
    )
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")
    
    return model


def train_ppo_topk(
    total_timesteps=100000,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    n_envs=4,
    top_k=20,
    save_path="models/ppo_topk_apple"
):
    """MaskablePPO + Top-K í•™ìŠµ
    
    íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ìƒìœ„ Kê°œ í›„ë³´ë§Œ ì„ íƒì§€ë¡œ ì œê³µ.
    Action spaceê°€ ì‘ì•„ì ¸ì„œ í•™ìŠµ íš¨ìœ¨ ì¦ê°€.
    """
    print("=" * 60)
    print(f"ğŸ§  MaskablePPO + Top-{top_k} í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    
    if not MASKABLE_AVAILABLE:
        print("âŒ sb3-contribê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sb3-contrib")
        return None
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (Top-K í™˜ê²½)
    env = DummyVecEnv([make_env_topk(rank=i, top_k=top_k) for i in range(n_envs)])
    
    # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ¯ Top-K: {top_k} (íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì„ ë³„ëœ ìƒìœ„ {top_k}ê°œ í›„ë³´ë§Œ ì„ íƒ ê°€ëŠ¥)")
    print(f"ğŸ­ Action Masking: í™œì„±í™”")
    print(f"ğŸ“Š ë‹¨ìˆœ ë³´ìƒ: cells + (ì¢…ë£Œ ì‹œ -remaining)")
    print(f"ğŸ‘ï¸ ê´€ì¸¡: ë³´ë“œ + Top-K í›„ë³´ íŠ¹ì§•(9ì°¨ì›) + ë§ˆìŠ¤í¬")
    
    # MaskablePPO ëª¨ë¸ ìƒì„±
    model = MaskablePPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.995,
        gae_lambda=0.97,
        clip_range=0.2,
        verbose=1,
        device=device
    )
    
    # ì½œë°± ì„¤ì •
    callback = LoggingCallback(log_freq=5000)
    
    # í•™ìŠµ
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì • (í™˜ê²½ {n_envs}ê°œ ë³‘ë ¬)")
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback
    )
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")
    
    return model, top_k


def evaluate_model(model, env, n_episodes=10):
    """í•™ìŠµëœ ëª¨ë¸ í‰ê°€ (envëŠ” make_env_topkë¡œ ìƒì„±ëœ wrapped env)"""
    scores = []
    steps_list = []
    
    for ep in range(n_episodes):
        obs, info = env.reset()
        
        while True:
            # ActionMaskerê°€ ì´ë¯¸ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ action_masks ë¶ˆí•„ìš”
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            if terminated or truncated:
                scores.append(info["total_score"])
                steps_list.append(info["steps"])
                break
    
    return {
        "mean_score": np.mean(scores),
        "std_score": np.std(scores),
        "max_score": np.max(scores),
        "min_score": np.min(scores),
        "mean_steps": np.mean(steps_list)
    }


def compare_with_heuristics(model, board_paths, verbose=True, top_k=20):
    """í•™ìŠµëœ ëª¨ë¸ê³¼ ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ë¹„êµ (make_env_topkì™€ ë™ì¼í•œ í™˜ê²½ ì‚¬ìš©)"""
    print("\n" + "=" * 60)
    print("ğŸ“Š RL ëª¨ë¸ vs íœ´ë¦¬ìŠ¤í‹± ë¹„êµ")
    print("=" * 60)
    
    results = {
        "RL Model": [],
        "Pair-First": [],
        "Full-Rollout": []
    }
    
    for board_path in board_paths:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # RL ëª¨ë¸: make_env_topkì™€ ë™ì¼í•œ wrapped í™˜ê²½ ì‚¬ìš©
        env = make_env_topk(top_k=top_k)()
        
        # ë¨¼ì € reset() í˜¸ì¶œí•˜ì—¬ Monitor ìƒíƒœ ì´ˆê¸°í™”
        env.reset()
        
        # ê·¸ í›„ ë³´ë“œë¥¼ ì›í•˜ëŠ” ë³´ë“œë¡œ êµì²´ (Monitor -> ActionMasker -> AppleGameEnvTopKFlat)
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.all_candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.top_candidates = unwrapped._select_top_k(unwrapped.all_candidates)
        unwrapped.prev_num_candidates = len(unwrapped.all_candidates)
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        while unwrapped.top_candidates:
            # ActionMaskerê°€ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ action_masks ë¶ˆí•„ìš”
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated:
                break
        results["RL Model"].append(info["total_score"])
        
        # Pair-First
        _, score_pf, _ = solve_pair_first(mat.copy(), verbose=False)
        results["Pair-First"].append(score_pf)
        
        # Full-Rollout
        _, score_fr, _ = solve_full_rollout(mat.copy(), top_k=30, verbose=False)
        results["Full-Rollout"].append(score_fr)
        
        if verbose:
            print(f"\n[{board_name}]")
            print(f"  RL Model:     {results['RL Model'][-1]:>4}")
            print(f"  Pair-First:   {results['Pair-First'][-1]:>4}")
            print(f"  Full-Rollout: {results['Full-Rollout'][-1]:>4}")
    
    # í‰ê·  ì¶œë ¥
    print("\n" + "-" * 60)
    print("ğŸ“ˆ í‰ê·  ì ìˆ˜:")
    for name, scores in results.items():
        print(f"  {name:<15}: {np.mean(scores):.1f} (Â±{np.std(scores):.1f})")
    
    return results


def play_with_model(model, board_path=None, render=True, top_k=20):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´ (make_env_topkì™€ ë™ì¼í•œ í™˜ê²½ ì‚¬ìš©)"""
    # make_env_topkì™€ ë™ì¼í•œ wrapped í™˜ê²½ ì‚¬ìš©
    env = make_env_topk(top_k=top_k)()
    unwrapped = env.unwrapped
    unwrapped.render_mode = "human" if render else None
    
    # ë¨¼ì € reset() í˜¸ì¶œí•˜ì—¬ Monitor ìƒíƒœ ì´ˆê¸°í™”
    obs, _ = env.reset()
    
    if board_path:
        # ë³´ë“œë¥¼ ì›í•˜ëŠ” ë³´ë“œë¡œ êµì²´
        unwrapped.board = read_matrix(board_path).astype(np.int32)
        unwrapped.all_candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.top_candidates = unwrapped._select_top_k(unwrapped.all_candidates)
        unwrapped.prev_num_candidates = len(unwrapped.all_candidates)
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
    
    if render:
        unwrapped.render()
    
    while True:
        # ActionMaskerê°€ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ action_masks ë¶ˆí•„ìš”
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        
        if render:
            unwrapped.render()
        
        if terminated or truncated:
            break
    
    print(f"\nğŸ ê²Œì„ ì¢…ë£Œ! ìµœì¢… ì ìˆ˜: {info['total_score']}, ìŠ¤í…: {info['steps']}")
    return info


def randomized_search(
    n_trials=20,
    timesteps_per_trial=50000,
    eval_episodes=30,
    save_best=True
):
    """
    ëœë¤ ì„œì¹˜ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    
    íƒìƒ‰ ë²”ìœ„:
    - top_k: [10, 20, 30, 50]
    - learning_rate: [1e-4, 3e-4, 1e-3]
    - n_steps: [1024, 2048, 4096]
    - batch_size: [32, 64, 128, 256]
    - n_epochs: [5, 10, 15, 20]
    - gamma: [0.99, 0.995, 0.999]
    - gae_lambda: [0.9, 0.95, 0.97, 0.99]
    """
    import random
    import json
    from datetime import datetime
    
    print("=" * 70)
    print("ğŸ” Randomized Search for Hyperparameter Optimization")
    print("=" * 70)
    
    # íƒìƒ‰ ê³µê°„ ì •ì˜
    search_space = {
        "top_k": [10, 20, 30, 50],
        "learning_rate": [1e-4, 3e-4, 5e-4, 1e-3],
        "n_steps": [1024, 2048, 4096],
        "batch_size": [32, 64, 128, 256],
        "n_epochs": [5, 10, 15, 20],
        "gamma": [0.99, 0.995, 0.999],
        "gae_lambda": [0.9, 0.95, 0.97, 0.99],
        "n_envs": [2, 4, 8]
    }
    
    print(f"ğŸ“‹ íƒìƒ‰ ê³µê°„:")
    for key, values in search_space.items():
        print(f"   {key}: {values}")
    print(f"\nğŸ² ì´ {n_trials}íšŒ ì‹œë„, ê° {timesteps_per_trial:,} ìŠ¤í…")
    print("-" * 70)
    
    # ê²°ê³¼ ì €ì¥
    results = []
    best_score = -float('inf')
    best_params = None
    best_model = None
    
    # ê³ ì • ë³´ë“œ íŒŒì¼ë¡œ í‰ê°€
    board_files = [f"board_mat/board{i}.txt" for i in range(1, 9)]
    board_files = [f for f in board_files if os.path.exists(f)]
    
    for trial in range(n_trials):
        print(f"\n{'='*70}")
        print(f"ğŸ¯ Trial {trial + 1}/{n_trials}")
        print("=" * 70)
        
        # ëœë¤ íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
        params = {key: random.choice(values) for key, values in search_space.items()}
        
        # batch_sizeê°€ n_steps * n_envsë³´ë‹¤ í¬ë©´ ì¡°ì •
        total_batch = params["n_steps"] * params["n_envs"]
        if params["batch_size"] > total_batch:
            params["batch_size"] = total_batch
        
        print(f"ğŸ“Š íŒŒë¼ë¯¸í„°: {params}")
        
        try:
            # í™˜ê²½ ìƒì„±
            env = DummyVecEnv([
                make_env_topk(rank=i, top_k=params["top_k"]) 
                for i in range(params["n_envs"])
            ])
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = MaskablePPO(
                "MlpPolicy",
                env,
                learning_rate=params["learning_rate"],
                n_steps=params["n_steps"],
                batch_size=params["batch_size"],
                n_epochs=params["n_epochs"],
                gamma=params["gamma"],
                gae_lambda=params["gae_lambda"],
                clip_range=0.2,
                verbose=0,
                device=device
            )
            
            model.learn(total_timesteps=timesteps_per_trial)
            
            # í‰ê°€: ê³ ì • ë³´ë“œì—ì„œ ì ìˆ˜ ì¸¡ì •
            eval_scores = []
            for board_path in board_files:
                mat = read_matrix(board_path)
                eval_env = make_env_topk(top_k=params["top_k"])()
                
                # reset í›„ ë³´ë“œ êµì²´
                eval_env.reset()
                unwrapped = eval_env.unwrapped
                unwrapped.board = mat.copy().astype(np.int32)
                unwrapped.all_candidates = list(find_candidates_fast(unwrapped.board))
                unwrapped.top_candidates = unwrapped._select_top_k(unwrapped.all_candidates)
                unwrapped.prev_num_candidates = len(unwrapped.all_candidates)
                unwrapped.total_score = 0
                unwrapped.steps = 0
                obs = unwrapped._get_obs()
                
                while unwrapped.top_candidates:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    if terminated:
                        break
                eval_scores.append(info["total_score"])
            
            # ëœë¤ ë³´ë“œì—ì„œë„ í‰ê°€
            random_env = make_env_topk(top_k=params["top_k"])()
            random_scores = []
            for _ in range(eval_episodes):
                obs, _ = random_env.reset()
                while True:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = random_env.step(action)
                    if terminated or truncated:
                        random_scores.append(info["total_score"])
                        break
            
            # ì ìˆ˜ ê³„ì‚° (ê³ ì • ë³´ë“œ + ëœë¤ ë³´ë“œ í‰ê· )
            fixed_mean = np.mean(eval_scores)
            random_mean = np.mean(random_scores)
            combined_score = 0.6 * fixed_mean + 0.4 * random_mean  # ê³ ì • ë³´ë“œ ì¤‘ì‹œ
            
            result = {
                "trial": trial + 1,
                "params": params,
                "fixed_board_mean": fixed_mean,
                "fixed_board_scores": eval_scores,
                "random_board_mean": random_mean,
                "combined_score": combined_score
            }
            results.append(result)
            
            print(f"âœ… ê³ ì • ë³´ë“œ í‰ê· : {fixed_mean:.1f}")
            print(f"âœ… ëœë¤ ë³´ë“œ í‰ê· : {random_mean:.1f}")
            print(f"âœ… ì¢…í•© ì ìˆ˜: {combined_score:.1f}")
            
            # ìµœê³  ì ìˆ˜ ê°±ì‹ 
            if combined_score > best_score:
                best_score = combined_score
                best_params = params.copy()
                best_model = model
                print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜! ({combined_score:.1f})")
                
        except Exception as e:
            print(f"âŒ Trial {trial + 1} ì‹¤íŒ¨: {e}")
            results.append({
                "trial": trial + 1,
                "params": params,
                "error": str(e)
            })
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š Randomized Search ê²°ê³¼")
    print("=" * 70)
    
    # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì •ë ¬
    successful = [r for r in results if "combined_score" in r]
    successful.sort(key=lambda x: x["combined_score"], reverse=True)
    
    print(f"\nğŸ… Top 5 ê²°ê³¼:")
    for i, r in enumerate(successful[:5], 1):
        print(f"\n{i}. ì¢…í•© ì ìˆ˜: {r['combined_score']:.1f}")
        print(f"   ê³ ì • ë³´ë“œ: {r['fixed_board_mean']:.1f}, ëœë¤ ë³´ë“œ: {r['random_board_mean']:.1f}")
        print(f"   íŒŒë¼ë¯¸í„°: {r['params']}")
    
    print(f"\nğŸ† ìµœì  íŒŒë¼ë¯¸í„°:")
    print(f"   {best_params}")
    print(f"   ìµœê³  ì ìˆ˜: {best_score:.1f}")
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs("search_results", exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON ì €ì¥
    results_file = f"search_results/search_{timestamp}.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump({
            "best_params": best_params,
            "best_score": best_score,
            "all_results": results
        }, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {results_file}")
    
    # ìµœê³  ëª¨ë¸ ì €ì¥
    if save_best and best_model:
        model_path = f"models/best_search_{timestamp}"
        best_model.save(model_path)
        print(f"ğŸ“ ìµœê³  ëª¨ë¸ ì €ì¥: {model_path}")
    
    return best_params, best_score, results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ")
    parser.add_argument("--mode", type=str, default="train", choices=["train", "eval", "play", "compare", "search"])
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--topk", type=int, default=20, help="Top-K í›„ë³´ ìˆ˜ (ê¸°ë³¸: 20)")
    parser.add_argument("--model", type=str, default=None, help="í‰ê°€/í”Œë ˆì´í•  ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--board", type=str, default=None, help="íŠ¹ì • ë³´ë“œë¡œ í”Œë ˆì´")
    parser.add_argument("--n_trials", type=int, default=20, help="ëœë¤ ì„œì¹˜ ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--trial_timesteps", type=int, default=50000, help="ì‹œë„ë‹¹ í•™ìŠµ ìŠ¤í…")
    args = parser.parse_args()
    
    if args.mode == "train":
        # í•™ìŠµ (MaskablePPO + Top-K)
        top_k_value = args.topk
        model, top_k_value = train_ppo_topk(total_timesteps=args.timesteps, top_k=args.topk)
        
        # í•™ìŠµ í›„ í‰ê°€: make_env_topkì™€ ë™ì¼í•œ í™˜ê²½ ì‚¬ìš©
        print("\nğŸ“Š í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ì¤‘...")
        eval_env = make_env_topk(top_k=top_k_value)()
        results = evaluate_model(model, eval_env, n_episodes=20)
        print(f"í‰ê·  ì ìˆ˜: {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"ìµœê³  ì ìˆ˜: {results['max_score']}")
        
        # íœ´ë¦¬ìŠ¤í‹±ê³¼ ë¹„êµ
        board_files = [f"board_mat/board{i}.txt" for i in range(1, 9)]
        board_files = [f for f in board_files if os.path.exists(f)]
        compare_with_heuristics(model, board_files, top_k=top_k_value)
        
    elif args.mode == "eval":
        # í‰ê°€ë§Œ
        model_path = args.model or "models/ppo_topk_apple"
        top_k_value = args.topk
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        eval_env = make_env_topk(top_k=top_k_value)()
        results = evaluate_model(model, eval_env, n_episodes=50)
        print(f"í‰ê·  ì ìˆ˜: {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"ìµœê³ /ìµœì €: {results['max_score']} / {results['min_score']}")
        
    elif args.mode == "play":
        # í”Œë ˆì´
        model_path = args.model or "models/ppo_topk_apple"
        top_k_value = args.topk
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        board_path = f"board_mat/{args.board}.txt" if args.board else None
        play_with_model(model, board_path, render=True, top_k=top_k_value)
        
    elif args.mode == "compare":
        # ë¹„êµ
        model_path = args.model or "models/ppo_topk_apple"
        top_k_value = args.topk
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        board_files = [f"board_mat/board{i}.txt" for i in range(1, 9)]
        board_files = [f for f in board_files if os.path.exists(f)]
        compare_with_heuristics(model, board_files, top_k=top_k_value)
    
    elif args.mode == "search":
        # ëœë¤ ì„œì¹˜
        print("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ëœë¤ ì„œì¹˜ ì‹œì‘...")
        best_params, best_score, results = randomized_search(
            n_trials=args.n_trials,
            timesteps_per_trial=args.trial_timesteps,
            eval_episodes=30,
            save_best=True
        )
        print(f"\nğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        print(f"ğŸ† ìµœê³  ì ìˆ˜: {best_score:.1f}")
