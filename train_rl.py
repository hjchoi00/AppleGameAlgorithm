"""
train_rl.py - ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

MaskablePPOë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ì „ëµ í•™ìŠµ
"""

import os
import random
import numpy as np
import torch
from datetime import datetime
import matplotlib.pyplot as plt


def set_seed(seed=42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ seed ê³ ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    print(f"ğŸ² Seed ê³ ì •: {seed}")


# Stable Baselines 3
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# MaskablePPO (Action Masking ì§€ì›)
try:
    from sb3_contrib import MaskablePPO
    from sb3_contrib.common.wrappers import ActionMasker
    MASKABLE_AVAILABLE = True
except ImportError:
    MASKABLE_AVAILABLE = False
    print("âš ï¸ sb3-contribê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sb3-contrib")

# í™˜ê²½
from apple_env import AppleGameEnv, AppleGameEnvWithMask

# ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ ë¹„êµìš©
from main import (
    read_matrix, find_candidates_fast, apply_move_fast,
    solve_pair_first, solve_full_rollout
)


class LoggingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥, Train/Val í‰ê°€ ë° Best ëª¨ë¸ ì €ì¥ ì½œë°±"""
    
    def __init__(self, log_freq=10000, eval_freq=5000, 
                 train_board_dir="board_mat/train",
                 val_board_dir="board_mat/val",
                 save_path="models/ppo_apple",
                 verbose=1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.eval_freq = eval_freq
        self.save_path = save_path
        self.episode_scores = []
        
        # timestep ê¸°ì¤€ ì¹´ìš´í„°
        self._next_log_timestep = log_freq
        self._next_eval_timestep = eval_freq
        
        # ê·¸ë˜í”„ìš© ë°ì´í„°
        self.timesteps_history = []
        self.train_avg_history = []  # Train ë³´ë“œ í‰ê·  ì ìˆ˜
        self.val_avg_history = []    # Val ë³´ë“œ í‰ê·  ì ìˆ˜
        
        # Best ëª¨ë¸ ì¶”ì  (Val ê¸°ì¤€)
        self.best_val_score = -float('inf')
        self.best_timestep = 0
        
        # Train ë³´ë“œ íŒŒì¼ ë¡œë“œ
        self.train_board_files = sorted([
            os.path.join(train_board_dir, f) 
            for f in os.listdir(train_board_dir) 
            if f.endswith(".txt")
        ])
        
        # Val ë³´ë“œ íŒŒì¼ ë¡œë“œ
        self.val_board_files = sorted([
            os.path.join(val_board_dir, f) 
            for f in os.listdir(val_board_dir) 
            if f.endswith(".txt")
        ])
        
        print(f"ğŸ“‹ Train ë³´ë“œ: {len(self.train_board_files)}ê°œ")
        print(f"ğŸ“‹ Val ë³´ë“œ: {len(self.val_board_files)}ê°œ")
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸°ë¡ (ë¡œê·¸ìš©)
        if self.locals.get("dones") is not None:
            for i, done in enumerate(self.locals["dones"]):
                if done:
                    info = self.locals["infos"][i]
                    if "total_score" in info:
                        self.episode_scores.append(info["total_score"])
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ë¡œê·¸ ì¶œë ¥ (num_timesteps ê¸°ì¤€)
        if self.num_timesteps >= self._next_log_timestep and self.episode_scores:
            avg_score = np.mean(self.episode_scores[-100:])
            print(f"[Step {self.num_timesteps:,}] ìµœê·¼ 100 ì—í”¼ì†Œë“œ í‰ê· : {avg_score:.1f}")
            self._next_log_timestep += self.log_freq
        
        # Train/Val ë³´ë“œ í‰ê°€ (num_timesteps ê¸°ì¤€)
        if self.num_timesteps >= self._next_eval_timestep:
            train_avg = self._evaluate_on_boards(self.train_board_files)
            val_avg = self._evaluate_on_boards(self.val_board_files)
            
            self.timesteps_history.append(self.num_timesteps)
            self.train_avg_history.append(train_avg)
            self.val_avg_history.append(val_avg)
            
            # Best ëª¨ë¸ ì €ì¥ (Val ê¸°ì¤€)
            is_best = val_avg > self.best_val_score
            if is_best:
                self.best_val_score = val_avg
                self.best_timestep = self.num_timesteps
                best_path = self.save_path + "_best"
                self.model.save(best_path)
                print(f"[Step {self.num_timesteps:,}] ğŸ† Train: {train_avg:.1f} | Val: {val_avg:.1f} â­ NEW BEST! ì €ì¥ë¨")
            else:
                print(f"[Step {self.num_timesteps:,}] ğŸ“Š Train: {train_avg:.1f} | Val: {val_avg:.1f} (best: {self.best_val_score:.1f} @ {self.best_timestep:,})")
            
            self._next_eval_timestep += self.eval_freq
        
        return True
    
    def _evaluate_on_boards(self, board_files):
        """ì§€ì •ëœ ë³´ë“œë“¤ì—ì„œ í˜„ì¬ ëª¨ë¸ í‰ê°€"""
        scores = []
        
        for board_path in board_files:
            mat = read_matrix(board_path)
            
            # í‰ê°€ìš© í™˜ê²½ ìƒì„± (seed=Noneìœ¼ë¡œ í•™ìŠµ RNGì— ì˜í–¥ ì—†ìŒ)
            env = make_env(use_mask=True)()
            env.reset()  # Monitorê°€ step í—ˆìš©í•˜ë„ë¡ reset í•„ìš”
            
            # ë³´ë“œ êµì²´ (reset ì§í›„ ë®ì–´ì“°ê¸°)
            unwrapped = env.unwrapped
            unwrapped.board = mat.copy().astype(np.int32)
            unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
            unwrapped.total_score = 0
            unwrapped.steps = 0
            unwrapped._compute_next_candidates_cache()
            obs = unwrapped._get_obs()
            
            # í”Œë ˆì´
            while True:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                if terminated or truncated:
                    break
            
            scores.append(info["total_score"])
        
        return np.mean(scores)
    
    def plot_learning_curve(self, save_path="learning_curve.png", show=True):
        """í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„± (Train/Val ë¹„êµ)"""
        import matplotlib as mpl
        import matplotlib.pyplot as plt

        # í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
        mpl.rcParams["font.family"] = "Malgun Gothic"
        mpl.rcParams["axes.unicode_minus"] = False
        
        if not self.timesteps_history:
            print("âš ï¸ ê¸°ë¡ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        plt.figure(figsize=(12, 6))
        
        # Train í‰ê·  ì ìˆ˜
        plt.plot(self.timesteps_history, self.train_avg_history, 
                 'b-o', linewidth=2, markersize=4, label=f'Train ({len(self.train_board_files)}ê°œ)')
        
        # Val í‰ê·  ì ìˆ˜
        plt.plot(self.timesteps_history, self.val_avg_history, 
                 'r-s', linewidth=2, markersize=4, label=f'Val ({len(self.val_board_files)}ê°œ)')
        
        # Best ì§€ì  í‘œì‹œ
        if self.best_timestep > 0:
            best_idx = self.timesteps_history.index(self.best_timestep) if self.best_timestep in self.timesteps_history else -1
            if best_idx >= 0:
                plt.axvline(x=self.best_timestep, color='g', linestyle='--', alpha=0.7, label=f'Best @ {self.best_timestep:,}')
                plt.scatter([self.best_timestep], [self.val_avg_history[best_idx]], 
                           color='g', s=100, zorder=5, marker='*')
        
        plt.xlabel('Timesteps', fontsize=12)
        plt.ylabel('Average Score', fontsize=12)
        plt.title('í•™ìŠµ ê³¡ì„  (Train vs Val)', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.legend()
        
        # Yì¶• ë²”ìœ„ ì¡°ì •
        all_scores = self.train_avg_history + self.val_avg_history
        if all_scores:
            y_min = min(all_scores) - 5
            y_max = max(all_scores) + 5
            plt.ylim(y_min, y_max)
        
        plt.tight_layout()
        
        # ì €ì¥
        plt.savefig(save_path, dpi=150)
        print(f"ğŸ“Š í•™ìŠµ ê³¡ì„  ì €ì¥: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()


def mask_fn(env):
    """í™˜ê²½ì—ì„œ action maskë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    return env.get_action_mask()


def make_env(board_dir="board_mat/train", rank=0, use_mask=False, seed=None):
    """í™˜ê²½ ìƒì„± í•¨ìˆ˜
    
    seedê°€ ì£¼ì–´ì§€ë©´ base_seedë¡œ ì„¤ì •ë˜ì–´
    ë§¤ ì—í”¼ì†Œë“œ resetë§ˆë‹¤ base_seed + episode_idxë¡œ ì¼ê´€ëœ seed ì‚¬ìš©
    """
    def _init():
        # base_seed ê³„ì‚°: seedê°€ ìˆìœ¼ë©´ seed + rank
        base_seed = (seed + rank) if seed is not None else None
        
        if use_mask:
            env = AppleGameEnvWithMask(board_dir=board_dir, base_seed=base_seed)
            env = ActionMasker(env, mask_fn)
        else:
            env = AppleGameEnv(board_dir=board_dir, base_seed=base_seed)
        
        return Monitor(env)
    return _init


def train_ppo(
    total_timesteps=100000,
    learning_rate=5e-5,
    n_steps=2048,
    batch_size=128,
    n_epochs=10,
    n_envs=2,
    save_path="models/ppo_apple",
    train_board_dir="board_mat/train",
    val_board_dir="board_mat/val",
    test_board_dir="board_mat/test"
):
    """MaskablePPO í•™ìŠµ (Action Masking ì ìš©) + Train/Val/Test ë¶„ë¦¬"""
    print("=" * 60)
    print("ğŸ§  MaskablePPO í•™ìŠµ ì‹œì‘ (Action Masking ì ìš©)")
    print("=" * 60)
    
    if not MASKABLE_AVAILABLE:
        print("âŒ sb3-contribê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sb3-contrib")
        return None
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (Train ë°ì´í„°ë¡œ í•™ìŠµ)
    env = DummyVecEnv([make_env(board_dir=train_board_dir, rank=i, use_mask=True, seed=42) for i in range(n_envs)])
    
    # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ­ Action Masking: í™œì„±í™” (ìœ íš¨í•œ í›„ë³´ë§Œ ì„ íƒ ê°€ëŠ¥)")
    
    # MaskablePPO ëª¨ë¸ ìƒì„± (Dict observation â†’ MultiInputPolicy)
    model = MaskablePPO(
        "MultiInputPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.999,
        gae_lambda=0.99,
        clip_range=0.2,
        verbose=1,
        device=device,
        seed=42  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ê³ ì •
    )
    
    # ì½œë°± ì„¤ì • (Train/Val í‰ê°€)
    eval_freq = 5000
    callback = LoggingCallback(
        log_freq=10000, 
        eval_freq=eval_freq,
        train_board_dir=train_board_dir,
        val_board_dir=val_board_dir,
        save_path=save_path
    )
    
    # í•™ìŠµ
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì • (í™˜ê²½ {n_envs}ê°œ ë³‘ë ¬)")
    print(f"ğŸ“Š Train/Val í‰ê°€ ì£¼ê¸°: {eval_freq:,} ìŠ¤í…ë§ˆë‹¤")
    print(f"ğŸ“ Train: {train_board_dir}")
    print(f"ğŸ“ Val: {val_board_dir}")
    print(f"ğŸ“ Test: {test_board_dir}")
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback
    )
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"\nâœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {save_path}")
    print(f"ğŸ† Best ëª¨ë¸ ì €ì¥: {save_path}_best (Val {callback.best_val_score:.1f} @ step {callback.best_timestep:,})")
    
    # í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„±
    callback.plot_learning_curve(
        save_path=save_path.replace(".zip", "") + "_learning_curve.png",
        show=True
    )
    
    # ========== Test í‰ê°€ (Best ëª¨ë¸ ì‚¬ìš©) ==========
    print("\n" + "=" * 60)
    print("ğŸ§ª Test ë°ì´í„° í‰ê°€ (Best ëª¨ë¸ ì‚¬ìš©)")
    print("=" * 60)
    
    best_model = MaskablePPO.load(save_path + "_best")
    compare_with_heuristics(best_model, board_dir=test_board_dir, verbose=True)
    
    return model, callback


def evaluate_model_on_all_boards(model, env_factory, board_dir="board_mat", verbose=False):
    """
    board_matì˜ ëª¨ë“  ë³´ë“œì—ì„œ í‰ê°€ (ê³ ì • ë³´ë“œ ì „ì²´ í‰ê°€)
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        env_factory: í™˜ê²½ ìƒì„± í•¨ìˆ˜
        board_dir: ë³´ë“œ íŒŒì¼ ë””ë ‰í† ë¦¬
        verbose: ê° ë³´ë“œë³„ ì ìˆ˜ ì¶œë ¥ ì—¬ë¶€
    """
    # ëª¨ë“  ë³´ë“œ íŒŒì¼ ë¡œë“œ
    board_files = sorted([
        os.path.join(board_dir, f) 
        for f in os.listdir(board_dir) 
        if f.endswith(".txt")
    ])
    
    if not board_files:
        print(f"âš ï¸ {board_dir}ì— ë³´ë“œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    scores = []
    steps_list = []
    
    for board_path in board_files:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # í™˜ê²½ ìƒì„± ë° ì´ˆê¸°í™”
        env = env_factory()
        env.reset()
        
        # ë³´ë“œ êµì²´
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        # í”Œë ˆì´
        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated or truncated:
                break
        
        scores.append(info["total_score"])
        steps_list.append(info["steps"])
        
        if verbose:
            print(f"  {board_name}: {info['total_score']} ì ")
    
    return {
        "mean_score": np.mean(scores),
        "std_score": np.std(scores),
        "max_score": np.max(scores),
        "min_score": np.min(scores),
        "mean_steps": np.mean(steps_list),
        "n_boards": len(board_files),
        "scores": scores
    }


def compare_with_heuristics(model, board_dir="board_mat", verbose=True):
    """í•™ìŠµëœ ëª¨ë¸ê³¼ ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ë¹„êµ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š RL ëª¨ë¸ vs íœ´ë¦¬ìŠ¤í‹± ë¹„êµ")
    print("=" * 60)
    
    board_files = sorted([
        os.path.join(board_dir, f) 
        for f in os.listdir(board_dir) 
        if f.endswith(".txt")
    ])
    
    results = {
        "RL Model": [],
        "Pair-First": [],
        "Full-Rollout": []
    }
    
    for board_path in board_files:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # RL ëª¨ë¸
        env = make_env(use_mask=True)()
        env.reset()
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        while unwrapped.candidates:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated:
                break
        results["RL Model"].append(info["total_score"])
        
        # Pair-First
        _, score_pf, _ = solve_pair_first(mat.copy(), verbose=False)
        results["Pair-First"].append(score_pf)
        
        # Full-Rollout
        _, score_fr, _ = solve_full_rollout(mat.copy(), top_k=30, verbose=False)
        results["Full-Rollout"].append(score_fr)
        
        if verbose:
            print(f"\n[{board_name}]")
            print(f"  RL Model:     {results['RL Model'][-1]:>4}")
            print(f"  Pair-First:   {results['Pair-First'][-1]:>4}")
            print(f"  Full-Rollout: {results['Full-Rollout'][-1]:>4}")
    
    # í‰ê·  ì¶œë ¥
    print("\n" + "-" * 60)
    print("ğŸ“ˆ í‰ê·  ì ìˆ˜:")
    for name, scores in results.items():
        print(f"  {name:<15}: {np.mean(scores):.1f} (Â±{np.std(scores):.1f})")
    
    return results


def play_with_model(model, board_path=None, render=True):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´"""
    env = make_env(use_mask=True)()
    unwrapped = env.unwrapped
    unwrapped.render_mode = "human" if render else None
    
    obs, _ = env.reset()
    
    if board_path:
        unwrapped.board = read_matrix(board_path).astype(np.int32)
        unwrapped.candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
    
    if render:
        unwrapped.render()
    
    while True:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        
        if render:
            unwrapped.render()
        
        if terminated or truncated:
            break
    
    print(f"\nğŸ ê²Œì„ ì¢…ë£Œ! ìµœì¢… ì ìˆ˜: {info['total_score']}, ìŠ¤í…: {info['steps']}")
    return info


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ")
    parser.add_argument("--mode", type=str, default="train", 
                        choices=["train", "eval", "play", "compare"])
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--model", type=str, default=None, help="í‰ê°€/í”Œë ˆì´í•  ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--board", type=str, default=None, help="íŠ¹ì • ë³´ë“œë¡œ í”Œë ˆì´")
    parser.add_argument("--seed", type=int, default=42, help="ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ (ê¸°ë³¸: 42, -1ì´ë©´ ëœë¤)")
    parser.add_argument("--test-dir", type=str, default="board_mat/test", help="í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ")
    args = parser.parse_args()
    
    # Seed ê³ ì • (-1ì´ë©´ ëœë¤)
    if args.seed >= 0:
        set_seed(args.seed)
    else:
        print("ğŸ² Seed: ëœë¤ (ê³ ì • ì•ˆí•¨)")
    
    if args.mode == "train":
        # í•™ìŠµ (MaskablePPO) - Train/Val/Test ë¶„ë¦¬
        print("=" * 60)
        print("ğŸ§  MaskablePPO í•™ìŠµ (Train/Val/Test ë¶„ë¦¬)")
        print("=" * 60)
        model, callback = train_ppo(total_timesteps=args.timesteps)
        # train_ppo ë‚´ì—ì„œ Test í‰ê°€ê¹Œì§€ ìˆ˜í–‰
        
    elif args.mode == "eval":
        # í‰ê°€ë§Œ (Test ë°ì´í„°)
        model_path = args.model or "models/ppo_apple_best"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        print(f"\nğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘ ({args.test_dir})...")
        compare_with_heuristics(model, board_dir=args.test_dir, verbose=True)
        
    elif args.mode == "play":
        # í”Œë ˆì´
        model_path = args.model or "models/ppo_apple_best"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        board_path = f"board_mat/test/{args.board}.txt" if args.board else None
        play_with_model(model, board_path, render=True)
        
    elif args.mode == "compare":
        # ë¹„êµ (Test ë°ì´í„°)
        model_path = args.model or "models/ppo_apple_best"
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        compare_with_heuristics(model, board_dir=args.test_dir)
