"""
train_rl.py - ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

MaskablePPOë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì  ì „ëµ í•™ìŠµ
"""

import os
import numpy as np
import torch
from datetime import datetime

# Stable Baselines 3
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.monitor import Monitor

# MaskablePPO (Action Masking ì§€ì›)
try:
    from sb3_contrib import MaskablePPO
    from sb3_contrib.common.wrappers import ActionMasker
    from sb3_contrib.common.maskable.callbacks import MaskableEvalCallback
    MASKABLE_AVAILABLE = True
except ImportError:
    MASKABLE_AVAILABLE = False
    print("âš ï¸ sb3-contribê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install sb3-contrib")

# í™˜ê²½
from apple_env import (
    AppleGameEnv, AppleGameEnvWithMask, AppleGameEnvTopK, AppleGameEnvTopKFlat,
    AppleGameEnvLearnedTopK, AppleGameEnvAllCandidates, CandidateScorer
)

# ê¸°ì¡´ ì•Œê³ ë¦¬ì¦˜ ë¹„êµìš©
from main import (
    read_matrix, find_candidates_fast, apply_move_fast,
    solve_pair_first, solve_full_rollout
)


class LoggingCallback(BaseCallback):
    """í•™ìŠµ ì¤‘ ë¡œê·¸ ì¶œë ¥ ì½œë°±"""
    
    def __init__(self, log_freq=1000, verbose=1):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.episode_rewards = []
        self.episode_scores = []
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸°ë¡
        if self.locals.get("dones") is not None:
            for i, done in enumerate(self.locals["dones"]):
                if done:
                    info = self.locals["infos"][i]
                    if "total_score" in info:
                        self.episode_scores.append(info["total_score"])
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì¶œë ¥
        if self.n_calls % self.log_freq == 0 and self.episode_scores:
            avg_score = np.mean(self.episode_scores[-100:])
            max_score = np.max(self.episode_scores[-100:]) if self.episode_scores else 0
            print(f"[Step {self.n_calls}] ìµœê·¼ 100 ì—í”¼ì†Œë“œ - í‰ê· : {avg_score:.1f}, ìµœê³ : {max_score}")
        
        return True


def mask_fn(env):
    """í™˜ê²½ì—ì„œ action maskë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    return env.get_action_mask()


def make_env(board_dir="board_mat", rank=0, use_mask=False):
    """í™˜ê²½ ìƒì„± í•¨ìˆ˜"""
    def _init():
        if use_mask:
            env = AppleGameEnvWithMask(board_dir=board_dir)
            # ActionMasker wrapper ì ìš© (ë§ˆìŠ¤í‚¹ ëª…ì‹œì  í™œì„±í™”)
            env = ActionMasker(env, mask_fn)
        else:
            env = AppleGameEnv(board_dir=board_dir)
        env = Monitor(env)
        return env
    return _init


def make_env_topk(board_dir="board_mat", rank=0, top_k=20):
    """Top-K í™˜ê²½ ìƒì„± í•¨ìˆ˜ (Flat ë²„ì „ - MLP Policy í˜¸í™˜)"""
    def _init():
        env = AppleGameEnvTopKFlat(board_dir=board_dir, top_k=top_k)
        # ActionMasker wrapper ì ìš©
        env = ActionMasker(env, mask_fn)
        env = Monitor(env)
        return env
    return _init


def train_ppo(
    total_timesteps=100000,
    learning_rate=0.0001,
    n_steps=2048,
    batch_size=128,
    n_epochs=10,
    n_envs=2,
    save_path="models/ppo_apple"
):
    """MaskablePPO í•™ìŠµ (Action Masking ì ìš©)"""
    print("=" * 60)
    print("ğŸ§  MaskablePPO í•™ìŠµ ì‹œì‘ (Action Masking ì ìš©)")
    print("=" * 60)
    
    if not MASKABLE_AVAILABLE:
        print("âŒ sb3-contribê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sb3-contrib")
        return None
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (Mask ì§€ì› í™˜ê²½)
    env = DummyVecEnv([make_env(rank=i, use_mask=True) for i in range(n_envs)])
    
    # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ­ Action Masking: í™œì„±í™” (ìœ íš¨í•œ í›„ë³´ë§Œ ì„ íƒ ê°€ëŠ¥)")
    
    # MaskablePPO ëª¨ë¸ ìƒì„±
    model = MaskablePPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.999,
        gae_lambda=0.99,
        clip_range=0.2,
        verbose=1,
        device=device
    )
    
    # ì½œë°± ì„¤ì •
    callback = LoggingCallback(log_freq=5000)
    
    # í•™ìŠµ
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì • (í™˜ê²½ {n_envs}ê°œ ë³‘ë ¬)")
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback
    )
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")
    
    return model


def train_ppo_topk(
    total_timesteps=100000,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    n_envs=4,
    top_k=20,
    save_path="models/ppo_topk_apple"
):
    """MaskablePPO + Top-K í•™ìŠµ
    
    íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ìƒìœ„ Kê°œ í›„ë³´ë§Œ ì„ íƒì§€ë¡œ ì œê³µ.
    Action spaceê°€ ì‘ì•„ì ¸ì„œ í•™ìŠµ íš¨ìœ¨ ì¦ê°€.
    """
    print("=" * 60)
    print(f"ğŸ§  MaskablePPO + Top-{top_k} í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    
    if not MASKABLE_AVAILABLE:
        print("âŒ sb3-contribê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install sb3-contrib")
        return None
    
    # ë³‘ë ¬ í™˜ê²½ ìƒì„± (Top-K í™˜ê²½)
    env = DummyVecEnv([make_env_topk(rank=i, top_k=top_k) for i in range(n_envs)])
    
    # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ¯ Top-K: {top_k} (íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì„ ë³„ëœ ìƒìœ„ {top_k}ê°œ í›„ë³´ë§Œ ì„ íƒ ê°€ëŠ¥)")
    print(f"ğŸ­ Action Masking: í™œì„±í™”")
    print(f"ğŸ“Š ë‹¨ìˆœ ë³´ìƒ: cells + (ì¢…ë£Œ ì‹œ -remaining)")
    print(f"ğŸ‘ï¸ ê´€ì¸¡: ë³´ë“œ + Top-K í›„ë³´ íŠ¹ì§•(9ì°¨ì›) + ë§ˆìŠ¤í¬")
    
    # MaskablePPO ëª¨ë¸ ìƒì„±
    model = MaskablePPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.995,
        gae_lambda=0.97,
        clip_range=0.2,
        verbose=1,
        device=device
    )
    
    # ì½œë°± ì„¤ì •
    callback = LoggingCallback(log_freq=5000)
    
    # í•™ìŠµ
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì • (í™˜ê²½ {n_envs}ê°œ ë³‘ë ¬)")
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback
    )
    
    # ëª¨ë¸ ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")
    
    return model, top_k


# ============================================================
# í•™ìŠµëœ Scorerë¡œ Top-K ì„ ë³„ (AIê°€ í›„ë³´ë¥¼ ë§Œë“œëŠ” ë°©ì‹)
# ============================================================

def make_env_all_candidates(board_dir="board_mat", rank=0, max_candidates=100):
    """ëª¨ë“  í›„ë³´ë¥¼ ì œê³µí•˜ëŠ” í™˜ê²½ ìƒì„±"""
    def _init():
        env = AppleGameEnvAllCandidates(board_dir=board_dir, max_candidates=max_candidates)
        env = ActionMasker(env, mask_fn)
        env = Monitor(env)
        return env
    return _init


def train_scorer_from_policy(
    policy_model,
    n_episodes=1000,
    max_candidates=100,
    save_path="models/candidate_scorer.pt"
):
    """
    í•™ìŠµëœ PPO policyì˜ action probabilityë¥¼ ì‚¬ìš©í•´ì„œ CandidateScorer í•™ìŠµ
    
    ì›ë¦¬:
    1. ëª¨ë“  í›„ë³´ì— ëŒ€í•´ policyì˜ action probabilityë¥¼ êµ¬í•¨
    2. ë†’ì€ í™•ë¥ ì„ ë°›ì€ í›„ë³´ = "ì¢‹ì€ í›„ë³´"ë¡œ ê°„ì£¼
    3. ì´ probabilityë¥¼ targetìœ¼ë¡œ Scorer ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµ
    """
    print("=" * 60)
    print("ğŸ“ CandidateScorer í•™ìŠµ (Policy â†’ Scorer ì§€ì‹ ì¦ë¥˜)")
    print("=" * 60)
    
    # Scorer ì´ˆê¸°í™” (6ê°œ íŠ¹ì§•: cells, next_candidates_ratio, r1, c1, r2, c2)
    scorer = CandidateScorer(input_dim=6, hidden_dim=64)
    optimizer = torch.optim.Adam(scorer.parameters(), lr=1e-3)
    criterion = torch.nn.MSELoss()
    
    # ë°ì´í„° ìˆ˜ì§‘ í™˜ê²½
    env = AppleGameEnvAllCandidates(board_dir="board_mat", max_candidates=max_candidates)
    
    all_features = []
    all_targets = []
    
    # ëª¨ë¸ ë””ë°”ì´ìŠ¤ í™•ì¸
    device = next(policy_model.policy.parameters()).device
    
    print(f"ğŸ“Š {n_episodes} ì—í”¼ì†Œë“œì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    for ep in range(n_episodes):
        obs, info = env.reset()
        
        while True:
            if not env.candidates:
                break
            
            # í˜„ì¬ í›„ë³´ë“¤ì˜ íŠ¹ì§• ì¶”ì¶œ (board ì „ë‹¬)
            features = scorer._extract_features(
                env.candidates, env.board, env.board_height, env.board_width
            )
            
            # Policyì˜ action probability êµ¬í•˜ê¸°
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)
            with torch.no_grad():
                # MaskablePPOì˜ policyì—ì„œ í™•ë¥  ì¶”ì¶œ
                dist = policy_model.policy.get_distribution(obs_tensor)
                action_probs = dist.distribution.probs[0].cpu().numpy()
            
            # ìœ íš¨í•œ í›„ë³´ì— ëŒ€í•œ í™•ë¥ ë§Œ ì¶”ì¶œ
            n_valid = len(env.candidates)
            valid_probs = action_probs[:n_valid]
            
            # ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
            if valid_probs.sum() > 0:
                valid_probs = valid_probs / valid_probs.sum()
            
            # ë°ì´í„° ì €ì¥
            all_features.append(features)
            all_targets.append(valid_probs)
            
            # ë‹¤ìŒ ìŠ¤í…
            action, _ = policy_model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            if terminated or truncated:
                break
        
        if (ep + 1) % 100 == 0:
            print(f"  ì—í”¼ì†Œë“œ {ep + 1}/{n_episodes} ì™„ë£Œ")
    
    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    print(f"\nğŸ”§ Scorer í•™ìŠµ ì¤‘...")
    
    # ê° ë°ì´í„°ë¥¼ ê°œë³„ ìƒ˜í”Œë¡œ ë³€í™˜
    X_list = []
    y_list = []
    for features, probs in zip(all_features, all_targets):
        for i in range(len(probs)):
            X_list.append(features[i])
            y_list.append(probs[i])
    
    X = torch.FloatTensor(np.array(X_list))
    y = torch.FloatTensor(np.array(y_list))
    
    print(f"  ì´ ìƒ˜í”Œ ìˆ˜: {len(X)}")
    
    # ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ
    batch_size = 256
    n_epochs = 50
    
    for epoch in range(n_epochs):
        indices = torch.randperm(len(X))
        total_loss = 0
        n_batches = 0
        
        for i in range(0, len(X), batch_size):
            batch_idx = indices[i:i+batch_size]
            batch_X = X[batch_idx]
            batch_y = y[batch_idx]
            
            # Forward
            pred = scorer(batch_X)
            loss = criterion(pred, batch_y)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            n_batches += 1
        
        if (epoch + 1) % 10 == 0:
            print(f"  Epoch {epoch + 1}/{n_epochs}, Loss: {total_loss / n_batches:.6f}")
    
    # ì €ì¥
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    torch.save(scorer.state_dict(), save_path)
    print(f"âœ… Scorer ì €ì¥: {save_path}")
    
    return scorer


def make_env_learned_topk(board_dir="board_mat", rank=0, top_k=20, scorer=None):
    """í•™ìŠµëœ Scorerë¥¼ ì‚¬ìš©í•˜ëŠ” Top-K í™˜ê²½ ìƒì„±"""
    def _init():
        env = AppleGameEnvLearnedTopK(
            board_dir=board_dir, 
            top_k=top_k, 
            scorer=scorer,
            train_scorer=False
        )
        env = ActionMasker(env, mask_fn)
        env = Monitor(env)
        return env
    return _init


def train_with_learned_scorer(
    scorer_path="models/candidate_scorer.pt",
    total_timesteps=100000,
    top_k=20,
    n_envs=4,
    save_path="models/ppo_learned_topk"
):
    """
    í•™ìŠµëœ Scorerë¡œ Top-Kë¥¼ ì„ ë³„í•˜ë©´ì„œ PPO í•™ìŠµ
    
    ê¸°ì¡´: ê·œì¹™ ê¸°ë°˜ Top-K â†’ PPO ì„ íƒ
    ìƒˆë¡œìš´: Scorer ê¸°ë°˜ Top-K â†’ PPO ì„ íƒ
    """
    print("=" * 60)
    print(f"ğŸ§  MaskablePPO + Learned Scorer Top-{top_k} í•™ìŠµ")
    print("=" * 60)
    
    # Scorer ë¡œë“œ
    scorer = CandidateScorer()
    scorer.load_state_dict(torch.load(scorer_path))
    scorer.eval()
    print(f"âœ… Scorer ë¡œë“œ: {scorer_path}")
    
    # í™˜ê²½ ìƒì„±
    env = DummyVecEnv([
        make_env_learned_topk(rank=i, top_k=top_k, scorer=scorer) 
        for i in range(n_envs)
    ])
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    print(f"ğŸ¯ Top-K: {top_k} (í•™ìŠµëœ Scorerë¡œ ì„ ë³„)")
    
    # MaskablePPO ëª¨ë¸ ìƒì„±
    model = MaskablePPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.995,
        gae_lambda=0.97,
        clip_range=0.2,
        verbose=1,
        device=device
    )
    
    callback = LoggingCallback(log_freq=5000)
    
    print(f"ì´ {total_timesteps:,} ìŠ¤í… í•™ìŠµ ì˜ˆì •")
    model.learn(total_timesteps=total_timesteps, callback=callback)
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    model.save(save_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {save_path}")
    
    return model, top_k, scorer


def evaluate_model(model, env, n_episodes=10):
    """í•™ìŠµëœ ëª¨ë¸ í‰ê°€ (envëŠ” make_env_topkë¡œ ìƒì„±ëœ wrapped env)"""
    scores = []
    steps_list = []
    
    for ep in range(n_episodes):
        obs, info = env.reset()
        
        while True:
            # ActionMaskerê°€ ì´ë¯¸ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ action_masks ë¶ˆí•„ìš”
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            if terminated or truncated:
                scores.append(info["total_score"])
                steps_list.append(info["steps"])
                break
    
    return {
        "mean_score": np.mean(scores),
        "std_score": np.std(scores),
        "max_score": np.max(scores),
        "min_score": np.min(scores),
        "mean_steps": np.mean(steps_list)
    }


def compare_with_heuristics(model, board_paths, verbose=True, top_k=20):
    """í•™ìŠµëœ ëª¨ë¸ê³¼ ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹± ë¹„êµ (make_env_topkì™€ ë™ì¼í•œ í™˜ê²½ ì‚¬ìš©)"""
    print("\n" + "=" * 60)
    print("ğŸ“Š RL ëª¨ë¸ vs íœ´ë¦¬ìŠ¤í‹± ë¹„êµ")
    print("=" * 60)
    
    results = {
        "RL Model": [],
        "Pair-First": [],
        "Full-Rollout": []
    }
    
    for board_path in board_paths:
        mat = read_matrix(board_path)
        board_name = os.path.basename(board_path)
        
        # RL ëª¨ë¸: make_env_topkì™€ ë™ì¼í•œ wrapped í™˜ê²½ ì‚¬ìš©
        env = make_env_topk(top_k=top_k)()
        
        # ë¨¼ì € reset() í˜¸ì¶œí•˜ì—¬ Monitor ìƒíƒœ ì´ˆê¸°í™”
        env.reset()
        
        # ê·¸ í›„ ë³´ë“œë¥¼ ì›í•˜ëŠ” ë³´ë“œë¡œ êµì²´ (Monitor -> ActionMasker -> AppleGameEnvTopKFlat)
        unwrapped = env.unwrapped
        unwrapped.board = mat.copy().astype(np.int32)
        unwrapped.all_candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.top_candidates = unwrapped._select_top_k(unwrapped.all_candidates)
        unwrapped.prev_num_candidates = len(unwrapped.all_candidates)
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
        
        while unwrapped.top_candidates:
            # ActionMaskerê°€ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ action_masks ë¶ˆí•„ìš”
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            if terminated:
                break
        results["RL Model"].append(info["total_score"])
        
        # Pair-First
        _, score_pf, _ = solve_pair_first(mat.copy(), verbose=False)
        results["Pair-First"].append(score_pf)
        
        # Full-Rollout
        _, score_fr, _ = solve_full_rollout(mat.copy(), top_k=30, verbose=False)
        results["Full-Rollout"].append(score_fr)
        
        if verbose:
            print(f"\n[{board_name}]")
            print(f"  RL Model:     {results['RL Model'][-1]:>4}")
            print(f"  Pair-First:   {results['Pair-First'][-1]:>4}")
            print(f"  Full-Rollout: {results['Full-Rollout'][-1]:>4}")
    
    # í‰ê·  ì¶œë ¥
    print("\n" + "-" * 60)
    print("ğŸ“ˆ í‰ê·  ì ìˆ˜:")
    for name, scores in results.items():
        print(f"  {name:<15}: {np.mean(scores):.1f} (Â±{np.std(scores):.1f})")
    
    return results


def play_with_model(model, board_path=None, render=True, top_k=20):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ ê²Œì„ í”Œë ˆì´ (make_env_topkì™€ ë™ì¼í•œ í™˜ê²½ ì‚¬ìš©)"""
    # make_env_topkì™€ ë™ì¼í•œ wrapped í™˜ê²½ ì‚¬ìš©
    env = make_env_topk(top_k=top_k)()
    unwrapped = env.unwrapped
    unwrapped.render_mode = "human" if render else None
    
    # ë¨¼ì € reset() í˜¸ì¶œí•˜ì—¬ Monitor ìƒíƒœ ì´ˆê¸°í™”
    obs, _ = env.reset()
    
    if board_path:
        # ë³´ë“œë¥¼ ì›í•˜ëŠ” ë³´ë“œë¡œ êµì²´
        unwrapped.board = read_matrix(board_path).astype(np.int32)
        unwrapped.all_candidates = list(find_candidates_fast(unwrapped.board))
        unwrapped.top_candidates = unwrapped._select_top_k(unwrapped.all_candidates)
        unwrapped.prev_num_candidates = len(unwrapped.all_candidates)
        unwrapped.total_score = 0
        unwrapped.steps = 0
        obs = unwrapped._get_obs()
    
    if render:
        unwrapped.render()
    
    while True:
        # ActionMaskerê°€ ì ìš©ë˜ì–´ ìˆìœ¼ë¯€ë¡œ action_masks ë¶ˆí•„ìš”
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, info = env.step(action)
        
        if render:
            unwrapped.render()
        
        if terminated or truncated:
            break
    
    print(f"\nğŸ ê²Œì„ ì¢…ë£Œ! ìµœì¢… ì ìˆ˜: {info['total_score']}, ìŠ¤í…: {info['steps']}")
    return info


def randomized_search(
    n_trials=20,
    timesteps_per_trial=50000,
    eval_episodes=30,
    save_best=True
):
    """
    ëœë¤ ì„œì¹˜ë¡œ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    
    íƒìƒ‰ ë²”ìœ„:
    - top_k: [10, 20, 30, 50]
    - learning_rate: [1e-4, 3e-4, 1e-3]
    - n_steps: [1024, 2048, 4096]
    - batch_size: [32, 64, 128, 256]
    - n_epochs: [5, 10, 15, 20]
    - gamma: [0.99, 0.995, 0.999]
    - gae_lambda: [0.9, 0.95, 0.97, 0.99]
    """
    import random
    import json
    from datetime import datetime
    
    print("=" * 70)
    print("ğŸ” Randomized Search for Hyperparameter Optimization")
    print("=" * 70)
    
    # íƒìƒ‰ ê³µê°„ ì •ì˜
    search_space = {
        "top_k": [10, 20, 30, 50],
        "learning_rate": [1e-4, 3e-4, 5e-4, 1e-3],
        "n_steps": [1024, 2048, 4096],
        "batch_size": [32, 64, 128, 256],
        "n_epochs": [5, 10, 15, 20],
        "gamma": [0.99, 0.995, 0.999],
        "gae_lambda": [0.9, 0.95, 0.97, 0.99],
        "n_envs": [2, 4, 8]
    }
    
    print(f"ğŸ“‹ íƒìƒ‰ ê³µê°„:")
    for key, values in search_space.items():
        print(f"   {key}: {values}")
    print(f"\nğŸ² ì´ {n_trials}íšŒ ì‹œë„, ê° {timesteps_per_trial:,} ìŠ¤í…")
    print("-" * 70)
    
    # ê²°ê³¼ ì €ì¥
    results = []
    best_score = -float('inf')
    best_params = None
    best_model = None
    
    # ê³ ì • ë³´ë“œ íŒŒì¼ë¡œ í‰ê°€ (board_mat í´ë”ì˜ ëª¨ë“  .txt íŒŒì¼)
    board_files = sorted([
        os.path.join("board_mat", f) 
        for f in os.listdir("board_mat") 
        if f.endswith(".txt")
    ])
    
    for trial in range(n_trials):
        print(f"\n{'='*70}")
        print(f"ğŸ¯ Trial {trial + 1}/{n_trials}")
        print("=" * 70)
        
        # ëœë¤ íŒŒë¼ë¯¸í„° ìƒ˜í”Œë§
        params = {key: random.choice(values) for key, values in search_space.items()}
        
        # batch_sizeê°€ n_steps * n_envsë³´ë‹¤ í¬ë©´ ì¡°ì •
        total_batch = params["n_steps"] * params["n_envs"]
        if params["batch_size"] > total_batch:
            params["batch_size"] = total_batch
        
        print(f"ğŸ“Š íŒŒë¼ë¯¸í„°: {params}")
        
        try:
            # í™˜ê²½ ìƒì„±
            env = DummyVecEnv([
                make_env_topk(rank=i, top_k=params["top_k"]) 
                for i in range(params["n_envs"])
            ])
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
            model = MaskablePPO(
                "MlpPolicy",
                env,
                learning_rate=params["learning_rate"],
                n_steps=params["n_steps"],
                batch_size=params["batch_size"],
                n_epochs=params["n_epochs"],
                gamma=params["gamma"],
                gae_lambda=params["gae_lambda"],
                clip_range=0.2,
                verbose=0,
                device=device
            )
            
            model.learn(total_timesteps=timesteps_per_trial)
            
            # í‰ê°€: ê³ ì • ë³´ë“œì—ì„œ ì ìˆ˜ ì¸¡ì •
            eval_scores = []
            for board_path in board_files:
                mat = read_matrix(board_path)
                eval_env = make_env_topk(top_k=params["top_k"])()
                
                # reset í›„ ë³´ë“œ êµì²´
                eval_env.reset()
                unwrapped = eval_env.unwrapped
                unwrapped.board = mat.copy().astype(np.int32)
                unwrapped.all_candidates = list(find_candidates_fast(unwrapped.board))
                unwrapped.top_candidates = unwrapped._select_top_k(unwrapped.all_candidates)
                unwrapped.prev_num_candidates = len(unwrapped.all_candidates)
                unwrapped.total_score = 0
                unwrapped.steps = 0
                obs = unwrapped._get_obs()
                
                while unwrapped.top_candidates:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = eval_env.step(action)
                    if terminated:
                        break
                eval_scores.append(info["total_score"])
            
            # ëœë¤ ë³´ë“œì—ì„œë„ í‰ê°€
            random_env = make_env_topk(top_k=params["top_k"])()
            random_scores = []
            for _ in range(eval_episodes):
                obs, _ = random_env.reset()
                while True:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = random_env.step(action)
                    if terminated or truncated:
                        random_scores.append(info["total_score"])
                        break
            
            # ì ìˆ˜ ê³„ì‚° (ê³ ì • ë³´ë“œ + ëœë¤ ë³´ë“œ í‰ê· )
            fixed_mean = np.mean(eval_scores)
            random_mean = np.mean(random_scores)
            combined_score = 0.6 * fixed_mean + 0.4 * random_mean  # ê³ ì • ë³´ë“œ ì¤‘ì‹œ
            
            result = {
                "trial": trial + 1,
                "params": params,
                "fixed_board_mean": fixed_mean,
                "fixed_board_scores": eval_scores,
                "random_board_mean": random_mean,
                "combined_score": combined_score
            }
            results.append(result)
            
            print(f"âœ… ê³ ì • ë³´ë“œ í‰ê· : {fixed_mean:.1f}")
            print(f"âœ… ëœë¤ ë³´ë“œ í‰ê· : {random_mean:.1f}")
            print(f"âœ… ì¢…í•© ì ìˆ˜: {combined_score:.1f}")
            
            # ìµœê³  ì ìˆ˜ ê°±ì‹ 
            if combined_score > best_score:
                best_score = combined_score
                best_params = params.copy()
                best_model = model
                print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì ìˆ˜! ({combined_score:.1f})")
                
        except Exception as e:
            print(f"âŒ Trial {trial + 1} ì‹¤íŒ¨: {e}")
            results.append({
                "trial": trial + 1,
                "params": params,
                "error": str(e)
            })
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ğŸ“Š Randomized Search ê²°ê³¼")
    print("=" * 70)
    
    # ì„±ê³µí•œ ê²°ê³¼ë§Œ ì •ë ¬
    successful = [r for r in results if "combined_score" in r]
    successful.sort(key=lambda x: x["combined_score"], reverse=True)
    
    print(f"\nğŸ… Top 5 ê²°ê³¼:")
    for i, r in enumerate(successful[:5], 1):
        print(f"\n{i}. ì¢…í•© ì ìˆ˜: {r['combined_score']:.1f}")
        print(f"   ê³ ì • ë³´ë“œ: {r['fixed_board_mean']:.1f}, ëœë¤ ë³´ë“œ: {r['random_board_mean']:.1f}")
        print(f"   íŒŒë¼ë¯¸í„°: {r['params']}")
    
    print(f"\nğŸ† ìµœì  íŒŒë¼ë¯¸í„°:")
    print(f"   {best_params}")
    print(f"   ìµœê³  ì ìˆ˜: {best_score:.1f}")
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs("search_results", exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # JSON ì €ì¥
    results_file = f"search_results/search_{timestamp}.json"
    with open(results_file, "w", encoding="utf-8") as f:
        json.dump({
            "best_params": best_params,
            "best_score": best_score,
            "all_results": results
        }, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {results_file}")
    
    # ìµœê³  ëª¨ë¸ ì €ì¥
    if save_best and best_model:
        model_path = f"models/best_search_{timestamp}"
        best_model.save(model_path)
        print(f"ğŸ“ ìµœê³  ëª¨ë¸ ì €ì¥: {model_path}")
    
    return best_params, best_score, results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ì‚¬ê³¼ê²Œì„ ê°•í™”í•™ìŠµ")
    parser.add_argument("--mode", type=str, default="train", 
                        choices=["train", "eval", "play", "compare", "search", "train_scorer", "train_learned"])
    parser.add_argument("--timesteps", type=int, default=100000)
    parser.add_argument("--topk", type=int, default=20, help="Top-K í›„ë³´ ìˆ˜ (ê¸°ë³¸: 20)")
    parser.add_argument("--model", type=str, default=None, help="í‰ê°€/í”Œë ˆì´í•  ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--scorer", type=str, default=None, help="Scorer ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--board", type=str, default=None, help="íŠ¹ì • ë³´ë“œë¡œ í”Œë ˆì´")
    parser.add_argument("--n_trials", type=int, default=20, help="ëœë¤ ì„œì¹˜ ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--trial_timesteps", type=int, default=50000, help="ì‹œë„ë‹¹ í•™ìŠµ ìŠ¤í…")
    parser.add_argument("--max_candidates", type=int, default=100, help="ì „ì²´ í›„ë³´ í™˜ê²½ì˜ ìµœëŒ€ í›„ë³´ ìˆ˜")
    args = parser.parse_args()
    
    if args.mode == "train":
        # í•™ìŠµ (MaskablePPO + Top-K)
        top_k_value = args.topk
        model, top_k_value = train_ppo_topk(total_timesteps=args.timesteps, top_k=args.topk)
        
        # í•™ìŠµ í›„ í‰ê°€: make_env_topkì™€ ë™ì¼í•œ í™˜ê²½ ì‚¬ìš©
        print("\nğŸ“Š í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ì¤‘...")
        eval_env = make_env_topk(top_k=top_k_value)()
        results = evaluate_model(model, eval_env, n_episodes=20)
        print(f"í‰ê·  ì ìˆ˜: {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"ìµœê³  ì ìˆ˜: {results['max_score']}")
        
        # íœ´ë¦¬ìŠ¤í‹±ê³¼ ë¹„êµ (board_mat í´ë”ì˜ ëª¨ë“  .txt íŒŒì¼)
        board_files = sorted([
            os.path.join("board_mat", f) 
            for f in os.listdir("board_mat") 
            if f.endswith(".txt")
        ])
        compare_with_heuristics(model, board_files, top_k=top_k_value)
        
    elif args.mode == "eval":
        # í‰ê°€ë§Œ
        model_path = args.model or "models/ppo_topk_apple"
        top_k_value = args.topk
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        eval_env = make_env_topk(top_k=top_k_value)()
        results = evaluate_model(model, eval_env, n_episodes=50)
        print(f"í‰ê·  ì ìˆ˜: {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
        print(f"ìµœê³ /ìµœì €: {results['max_score']} / {results['min_score']}")
        
    elif args.mode == "play":
        # í”Œë ˆì´
        model_path = args.model or "models/ppo_topk_apple"
        top_k_value = args.topk
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        board_path = f"board_mat/{args.board}.txt" if args.board else None
        play_with_model(model, board_path, render=True, top_k=top_k_value)
        
    elif args.mode == "compare":
        # ë¹„êµ
        model_path = args.model or "models/ppo_topk_apple"
        top_k_value = args.topk
        
        if MASKABLE_AVAILABLE:
            model = MaskablePPO.load(model_path)
        else:
            model = PPO.load(model_path)
        
        board_files = sorted([
            os.path.join("board_mat", f) 
            for f in os.listdir("board_mat") 
            if f.endswith(".txt")
        ])
        compare_with_heuristics(model, board_files, top_k=top_k_value)
    
    elif args.mode == "search":
        # ëœë¤ ì„œì¹˜
        print("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ëœë¤ ì„œì¹˜ ì‹œì‘...")
        best_params, best_score, results = randomized_search(
            n_trials=args.n_trials,
            timesteps_per_trial=args.trial_timesteps,
            eval_episodes=30,
            save_best=True
        )
        print(f"\nğŸ† ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        print(f"ğŸ† ìµœê³  ì ìˆ˜: {best_score:.1f}")
    
    elif args.mode == "train_scorer":
        # Step 1: ëª¨ë“  í›„ë³´ í™˜ê²½ì—ì„œ PPO í•™ìŠµ
        print("=" * 70)
        print("ğŸ¯ Step 1: ëª¨ë“  í›„ë³´ í™˜ê²½ì—ì„œ PPO í•™ìŠµ")
        print("=" * 70)
        
        # ëª¨ë“  í›„ë³´ í™˜ê²½ìœ¼ë¡œ í•™ìŠµ
        env = DummyVecEnv([
            make_env_all_candidates(max_candidates=args.max_candidates) 
            for _ in range(4)
        ])
        
        device = "cuda" if torch.cuda.is_available() else "cpu"
        base_model = MaskablePPO(
            "MlpPolicy",
            env,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.995,
            gae_lambda=0.97,
            verbose=1,
            device=device
        )
        
        print(f"ì´ {args.timesteps:,} ìŠ¤í… í•™ìŠµ (ì „ì²´ í›„ë³´ í™˜ê²½)")
        base_model.learn(total_timesteps=args.timesteps, callback=LoggingCallback(log_freq=5000))
        base_model.save("models/ppo_all_candidates")
        print("âœ… ê¸°ë°˜ ëª¨ë¸ ì €ì¥: models/ppo_all_candidates")
        
        # Step 2: Policyì—ì„œ Scorer í•™ìŠµ
        print("\n" + "=" * 70)
        print("ğŸ¯ Step 2: Policy â†’ Scorer ì§€ì‹ ì¦ë¥˜")
        print("=" * 70)
        
        scorer = train_scorer_from_policy(
            base_model,
            n_episodes=500,
            max_candidates=args.max_candidates,
            save_path="models/candidate_scorer.pt"
        )
        
        print("\nâœ… Scorer í•™ìŠµ ì™„ë£Œ!")
        print("ë‹¤ìŒ ë‹¨ê³„: python train_rl.py --mode train_learned --topk 20")
    
    elif args.mode == "train_learned":
        # í•™ìŠµëœ Scorerë¡œ Top-K ì„ ë³„í•˜ë©´ì„œ PPO í•™ìŠµ
        scorer_path = args.scorer or "models/candidate_scorer.pt"
        
        if not os.path.exists(scorer_path):
            print(f"âŒ Scorer íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {scorer_path}")
            print("ë¨¼ì € --mode train_scorerë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        else:
            model, top_k_value, scorer = train_with_learned_scorer(
                scorer_path=scorer_path,
                total_timesteps=args.timesteps,
                top_k=args.topk,
                save_path="models/ppo_learned_topk"
            )
            
            # í‰ê°€
            print("\nğŸ“Š í•™ìŠµëœ ëª¨ë¸ í‰ê°€ ì¤‘...")
            eval_env = make_env_learned_topk(top_k=top_k_value, scorer=scorer)()
            results = evaluate_model(model, eval_env, n_episodes=20)
            print(f"í‰ê·  ì ìˆ˜: {results['mean_score']:.1f} (Â±{results['std_score']:.1f})")
            print(f"ìµœê³  ì ìˆ˜: {results['max_score']}")
